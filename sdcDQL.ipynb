{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import cv2\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from math import *\n",
    "sys.path.append(\"./config\")\n",
    "import config\n",
    "from config import Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which map\n",
    "MAP_PATH = os.path.join('Assets', 'map3.png')\n",
    "\n",
    "\n",
    "# Init Car object\n",
    "Car = Car()\n",
    "# Initialize the window\n",
    "WIN = pygame.display.set_mode((config.WIDTH, config.HEIGHT))\n",
    "pygame.display.set_caption('SDC-RL')\n",
    "\n",
    "# Initalize fonts for text\n",
    "pygame.font.init()\n",
    "REWARD_FONT = pygame.font.SysFont('comicsans', 30)\n",
    "INPUT_FONT  = pygame.font.SysFont('comicsans', 15)\n",
    "\n",
    "# Load in map of our choosing\n",
    "MAP_IMAGE = pygame.image.load(MAP_PATH).convert_alpha()\n",
    "\n",
    "# Initialize the car image\n",
    "# NB: Convert converts to pixel and speeds up runtime\n",
    "CAR_IMAGE = pygame.image.load(os.path.join('Assets', 'car.png')).convert_alpha()\n",
    "# Car starts facing positive x-axis\n",
    "CAR_IMAGE = pygame.transform.rotate(pygame.transform.scale(CAR_IMAGE, (Car.width, Car.height)), -90)\n",
    "\n",
    "# Define all the event IDs\n",
    "COLLISION   = pygame.USEREVENT+1\n",
    "REWARD      = pygame.USEREVENT+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Environment for Q learning\n",
    "\n",
    "Action Space:       forward, backward, right, left, nothing\n",
    "Observation Space:  [Car.x, Car.y, Car.ang, Car.vel, laserscan]\n",
    "                    First array argument represents all lowerbound for each observation\n",
    "                    Second is all upperbound for each observation\n",
    "State:              [Car.x, Car.y, Car.ang, Car.vel, laserscan]\n",
    "                    TODO: Ask if this is supposed to be same as observation space\n",
    "\n",
    "'''\n",
    "class SDCEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.action_space = Discrete(5)\n",
    "        self.num_obs = Car.num_laserscan + 4\n",
    "        ls_low  = np.full((1, Car.num_laserscan), 0)[0] #Laserscan low\n",
    "        ls_high = np.full((1, Car.num_laserscan), Car.laserscan_dist)[0] #Laserscan high\n",
    "        plow    = np.array([0, 0, -np.inf, -config.VEL_MAX])\n",
    "        phigh   = np.array([config.WIDTH, config.HEIGHT, np.inf, config.VEL_MAX])\n",
    "\n",
    "        low = np.array([np.float32(np.append(plow, ls_low))])\n",
    "        high = np.array([np.float32(np.append(phigh, ls_high))])\n",
    "        self.observation_space = Box(low=low,\n",
    "                                    high=high)\n",
    "        self.ls_def     = np.full((1, Car.num_laserscan), -1)[0] #Laserscan default\n",
    "        self.state      = [Car.x, Car.y, Car.ang, Car.vel]\n",
    "        self.state.extend(list(self.ls_def)) # extend returns None\n",
    "        self.game_car   = pygame.Rect(0, 0, Car.width, Car.height)\n",
    "        self.WALLS      = self.create_walls()\n",
    "        self.WAYPOINTS  = self.create_waypoints()\n",
    "        self.laserscan  = []\n",
    "        self.impactxy   = []\n",
    "        # information needed for visualization\n",
    "        self._action    = -1  # Store last action so that we can visualize inputs\n",
    "        self._reward    = 0.   # Store total reward to display on screen\n",
    "\n",
    "    '''\n",
    "    Step function:      Executes action and calulates reward\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    action:             Action from action space\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    state:              New state after action, our Car object\n",
    "    reward:             Reward for taking that action\n",
    "    done:               Whether the episode is over, if our car crashes\n",
    "    info:               Debug information\n",
    "    '''\n",
    "    def step(self, action):\n",
    "        self._action = action # Store for visualization\n",
    "        # Apply the action\n",
    "        self.handle_movement(action)\n",
    "        self.check_velocity()\n",
    "        Car.x  += Car.vel*cos(Car.ang)\n",
    "        Car.y  += Car.vel*sin(Car.ang)\n",
    "        self.game_car.centerx = Car.x\n",
    "        self.game_car.centery = Car.y\n",
    "        self.detect_wall_collision(self.WALLS)\n",
    "        self.detect_waypoint_collision(self.game_car, self.WAYPOINTS)\n",
    "        self.laserscan, self.impactxy = self.get_laserscan(self.WALLS)\n",
    "\n",
    "        # Check if Car interacted with anything\n",
    "        # Calculate reward from this: -.01 for nothing, 10 for waypoint, -50 for collision\n",
    "        reward = -.01\n",
    "        done = False\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == REWARD:\n",
    "                reward  = 10\n",
    "            if event.type == COLLISION:\n",
    "                done    = True\n",
    "                reward  = -50\n",
    "        \n",
    "        self._reward += reward  # For visualization\n",
    "        # Set the new state\n",
    "        self.state = [Car.x, Car.y, Car.ang, Car.vel]\n",
    "        self.state.extend(self.laserscan)\n",
    "\n",
    "        # Placeholder for information\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        angle_in_degrees = Car.ang*(180./pi)\n",
    "        image    = pygame.transform.rotate(CAR_IMAGE, -angle_in_degrees)\n",
    "        self.game_car = image.get_rect(center=self.game_car.center)\n",
    "        # Draw background\n",
    "        WIN.blit(MAP_IMAGE, (0,0))\n",
    "        # Draw reward text\n",
    "        reward_text = REWARD_FONT.render(\"Reward: \"+str(round(self._reward,3)),\n",
    "                                         1, config.GREEN)\n",
    "        WIN.blit(reward_text, (config.WIDTH - reward_text.get_width()-10, 10))\n",
    "        # Draw input indicators\n",
    "        self.draw_indicators(self._action)\n",
    "        # Draw Walls different color\n",
    "        #for wall in WALLS:\n",
    "            #pygame.draw.rect(WIN, config.ORANGE, wall)\n",
    "\n",
    "        # Draw Car hitbox\n",
    "        pygame.draw.rect(WIN, config.SOFT_RED, self.game_car)\n",
    "        # Draw the laserscan\n",
    "        self.draw_laserscan(self.laserscan, self.impactxy)\n",
    "        # Draw car\n",
    "        WIN.blit(image, image.get_rect(center=(Car.x, Car.y)))\n",
    "\n",
    "        pygame.display.update()\n",
    "    def reset(self):\n",
    "        self._reward = 0\n",
    "        self.game_car.x = config.STARTX\n",
    "        self.game_car.y = config.STARTX\n",
    "        Car.reset()\n",
    "        self.state = [Car.x, Car.y, Car.ang, Car.vel]\n",
    "        self.state.extend(list(self.ls_def))\n",
    "        return self.state\n",
    "\n",
    "    '''\n",
    "    Function to draw the laserscan in draw_window\n",
    "    '''\n",
    "    def draw_laserscan(self, laserscan, impactxy):\n",
    "        num     = Car.num_laserscan\n",
    "        angle   = Car.ang\n",
    "        if not laserscan:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(0, num):\n",
    "                # Find the start and endline using trig (similar to forward kinematics)\n",
    "                # If it equals negative 1 the full laser length is drawn\n",
    "                if laserscan[i] == -1:\n",
    "                    pygame.draw.line(WIN, config.RED,\n",
    "                                    (Car.x, Car.y),\n",
    "                                    (Car.x+(Car.laserscan_dist*cos(angle)), Car.y+(Car.laserscan_dist*sin(angle))),\n",
    "                                    1)\n",
    "                else:\n",
    "                    pygame.draw.line(WIN, config.RED,\n",
    "                                    (Car.x, Car.y),\n",
    "                                    (Car.x+(laserscan[i]*cos(angle)), Car.y+(laserscan[i]*sin(angle))),\n",
    "                                    1)\n",
    "                angle += ((2*pi) / num)\n",
    "            # draw small circle at impact point of laser and obstacle\n",
    "            for x,y in impactxy:\n",
    "                pygame.draw.circle(WIN, config.RED, (x,y), 3, 3)\n",
    "\n",
    "    '''\n",
    "    Function to draw indicators\n",
    "    '''\n",
    "    def draw_indicators(self, action):\n",
    "        # xy of where we put the indicators\n",
    "        space = 35\n",
    "        leftxy    = (config.WIDTH/2 - space, config.HEIGHT/2)\n",
    "        rightxy  = (config.WIDTH/2 + space, config.HEIGHT/2)\n",
    "        upxy = (config.WIDTH/2, config.HEIGHT/2-space)\n",
    "        downxy  = (config.WIDTH/2, config.HEIGHT/2)\n",
    "\n",
    "        lefttext    = INPUT_FONT.render(\"A\", 1, config.BLACK)\n",
    "        righttext   = INPUT_FONT.render(\"D\", 1, config.BLACK)\n",
    "        uptext      = INPUT_FONT.render(\"W\", 1, config.BLACK)\n",
    "        downtext    = INPUT_FONT.render(\"S\", 1, config.BLACK)\n",
    "\n",
    "        # center text\n",
    "        lefttext_rec    = lefttext.get_rect(center=leftxy)\n",
    "        righttext_rec   = righttext.get_rect(center=rightxy)\n",
    "        uptext_rec      = uptext.get_rect(center=upxy)\n",
    "        downtext_rec    = downtext.get_rect(center=downxy)\n",
    "\n",
    "        # size\n",
    "        press_size  = 14\n",
    "        rest_size   = 15\n",
    "\n",
    "        color_fill = config.TAN\n",
    "        color_border = config.ORANGE\n",
    "\n",
    "        if action==0:  # LEFT\n",
    "            pygame.draw.circle(WIN, color_fill, list(leftxy), press_size, 0)\n",
    "            WIN.blit(lefttext, lefttext_rec)\n",
    "        else:\n",
    "            pygame.draw.circle(WIN, color_border, list(leftxy), rest_size, 3)\n",
    "            WIN.blit(lefttext, lefttext_rec)\n",
    "        if action==1:  # RIGHT\n",
    "            pygame.draw.circle(WIN, color_fill, list(rightxy), press_size, 0)\n",
    "            WIN.blit(righttext, righttext_rec)\n",
    "        else:\n",
    "            pygame.draw.circle(WIN, color_border, list(rightxy), rest_size, 3)\n",
    "            WIN.blit(righttext, righttext_rec)\n",
    "        if action==2:  # UP\n",
    "            pygame.draw.circle(WIN, color_fill, list(upxy), press_size, 0)\n",
    "            WIN.blit(uptext, uptext_rec)\n",
    "        else:\n",
    "            pygame.draw.circle(WIN, color_border, list(upxy), rest_size, 3)\n",
    "            WIN.blit(uptext, uptext_rec)\n",
    "        if action==3:  # DOWN\n",
    "            pygame.draw.circle(WIN, color_fill, list(downxy), press_size, 0)\n",
    "            WIN.blit(downtext, downtext_rec)\n",
    "        else:\n",
    "            pygame.draw.circle(WIN, color_border, list(downxy), rest_size, 3)\n",
    "            WIN.blit(downtext, downtext_rec)\n",
    "\n",
    "    '''\n",
    "    Function to find all the walls from the background image and\n",
    "    store them in the WALLS list\n",
    "\n",
    "    Using openCV line detection for black lines\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Coordinates of the begin/end points of each line segment that makes up each barrier\n",
    "    '''\n",
    "    def create_walls(self):\n",
    "        print('Generating Walls...')\n",
    "        # Preprocessing\n",
    "        img = cv2.imread(MAP_PATH, cv2.IMREAD_COLOR)\n",
    "        lower = np.array([0, 0, 0])\n",
    "        upper = np.array([0, 0, 0])\n",
    "        black_mask = cv2.inRange(img, lower, upper) # Isolate all black pixels\n",
    "        result = 255 - black_mask\n",
    "\n",
    "        low_threshold = 50\n",
    "        high_threshold = 150\n",
    "        edges = cv2.Canny(result, low_threshold, high_threshold)\n",
    "        dilated = cv2.dilate(edges, np.ones((3,3), dtype=np.uint8))\n",
    "\n",
    "        rho = 1  # distance resolution in pixels of the Hough grid\n",
    "        theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "        threshold = 25  # minimum number of votes (intersections in Hough grid cell)\n",
    "        min_line_length = 50  # minimum number of pixels making up a line\n",
    "        max_line_gap = 20  # maximum gap in pixels between connectable line segments\n",
    "        line_image = np.copy(img) * 0  # creating a blank to draw lines on\n",
    "\n",
    "        # Run Hough on edge detected image\n",
    "        # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "        lines = cv2.HoughLinesP(dilated, rho, theta, threshold, np.array([]),\n",
    "                            min_line_length, max_line_gap)\n",
    "\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),1)\n",
    "        lines_edges = cv2.addWeighted(img, 0.8, line_image, 1, 0)\n",
    "        #cv2.imshow('Edges', dilated)\n",
    "        cv2.imshow('Detected Walls/Obstacles', line_image)\n",
    "        cv2.waitKey(1)\n",
    "        print(str(len(lines))+' lines detected')\n",
    "        return lines\n",
    "\n",
    "    '''\n",
    "    Function to create reward waypoints\n",
    "\n",
    "    Need to use Blue RGB (0,0, 255) circles in drawing to signify a waypoint\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Pygame rectangles that approximate the waypoint\n",
    "    '''\n",
    "    def create_waypoints(self):\n",
    "        print('Generating Waypoints...')\n",
    "        # Preprocessing\n",
    "        img = cv2.imread(MAP_PATH, cv2.IMREAD_COLOR)\n",
    "        lower = np.array([255, 0, 0])\n",
    "        upper = np.array([255, 0, 0])\n",
    "        blue_mask = cv2.inRange(img, lower, upper) # Isolate all blue pixels\n",
    "        result = cv2.bitwise_and(img, img, mask=blue_mask)\n",
    "\n",
    "\n",
    "        gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.medianBlur(gray, 1)\n",
    "        rows = gray.shape[0]\n",
    "        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, rows/8,\n",
    "                                    param1=100, param2=20, minRadius=0, maxRadius=200)\n",
    "\n",
    "        WAYPOINTS = []\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles))\n",
    "            print(str(len(circles[0,:]))+' waypoints detected')\n",
    "            for i in circles[0, :]:\n",
    "                center = (i[0], i[1])\n",
    "                # circle center\n",
    "                cv2.circle(img, center, 1, (0, 100, 100), 3)\n",
    "                # circle outline\n",
    "                radius = i[2]\n",
    "                cv2.circle(img, center, radius, (255, 0, 255), 3)\n",
    "                WAYPOINTS.append(pygame.Rect(center, (radius, radius)))\n",
    "        else:\n",
    "            print('0 waypoints detected')\n",
    "\n",
    "        cv2.imshow('Detected Waypoints (in purple)', img)\n",
    "        cv2.waitKey(1)\n",
    "        return WAYPOINTS\n",
    "\n",
    "    def handle_movement(self,action):\n",
    "        if action == 0:                 # LEFT\n",
    "            Car.ang     -= Car.w\n",
    "        if action == 1:                 # RIGHT\n",
    "            Car.ang     += Car.w\n",
    "        if action == 2:                 # UP\n",
    "            Car.vel     += Car.acc\n",
    "        if action == 3:                 # DOWN\n",
    "            Car.vel     -= Car.acc\n",
    "        if action == 4:                 # Nothing\n",
    "            pass\n",
    "\n",
    "    '''\n",
    "    Function that checks velocity for max velocity and adds friction damping term\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Nothing\n",
    "    '''\n",
    "    def check_velocity(self):\n",
    "        # Check if velocity exceeds max velocity\n",
    "        if Car.vel > config.VEL_MAX:\n",
    "            Car.vel = config.VEL_MAX\n",
    "        elif Car.vel < -config.VEL_MAX:\n",
    "            Car.vel = -config.VEL_MAX\n",
    "        \n",
    "        # Apply friction damping term\n",
    "        if Car.vel > 0:\n",
    "            if Car.vel - config.FRICTION < 0:\n",
    "                Car.vel = 0\n",
    "            else:\n",
    "                Car.vel -= config.FRICTION\n",
    "        if Car.vel < 0:\n",
    "            if Car.vel + config.FRICTION > 0:\n",
    "                Car.vel = 0\n",
    "            else:\n",
    "                Car.vel += config.FRICTION\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function to detect collision with walls based on line intersection\n",
    "    we break our car down into four line segments and check against walls\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    WALLS:  List of all our line segment walls\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    Nothing\n",
    "    '''\n",
    "    def detect_wall_collision(self, WALLS):\n",
    "        bl = ((Car.x-(cos(Car.ang)*Car.height/2)), (Car.y-(sin(Car.ang)*Car.width/2))) # back left point of car \n",
    "        fl = ((Car.x+(cos(Car.ang)*Car.height/2)), (Car.y-(sin(Car.ang)*Car.width/2))) # front left point of car \n",
    "        br = ((Car.x-(cos(Car.ang)*Car.height/2)), (Car.y+(sin(Car.ang)*Car.width/2))) # back right point of car \n",
    "        fr = ((Car.x+(cos(Car.ang)*Car.height/2)), (Car.y+(sin(Car.ang)*Car.width/2))) # front right point of car \n",
    "\n",
    "        front   = (fl,fr)\n",
    "        back    = (bl, br)\n",
    "        lside   = (fl, bl)\n",
    "        rside   = (fr, br)\n",
    "        car_seg = [front, back, lside, rside]\n",
    "\n",
    "        intersection = False\n",
    "        for wall in WALLS:\n",
    "            for x3,y3,x4,y4 in wall:\n",
    "                for seg in car_seg:\n",
    "                    x1 = seg[0][0]\n",
    "                    y1 = seg[0][1]\n",
    "                    x2 = seg[1][0]\n",
    "                    y2 = seg[1][1]\n",
    "                    denom  = (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4)\n",
    "                    # if denom 0, lines parallel so never intersect\n",
    "                    if denom == 0:\n",
    "                        continue\n",
    "                    t1  = (x1-x3)*(y3-y4) - (y1-y3)*(x3-x4)\n",
    "                    t   = t1/denom\n",
    "                    u1  = (x1-x3)*(y1-y2) - (y1-y3)*(x1-x2)\n",
    "                    u   = u1/denom\n",
    "                    # Test to see if intersection exists\n",
    "                    if 0<=t and t<=1 and 0<=u and u<=1:\n",
    "                        intersection = True\n",
    "                        pygame.event.post(pygame.event.Event(COLLISION))\n",
    "                        break # Stop checking for wall intersection if we already found one\n",
    "            if intersection:\n",
    "                break\n",
    "    \n",
    "    '''\n",
    "    Function to detect whether we have reached a waypoint\n",
    "    We can hit a waypoint once. It will refresh after we hit everyother waypoint\n",
    "    TODO: This is messy but whatever\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    game_car:   pygame Rect for collision detection\n",
    "    WAYPOINTS:  Rect objects representing our waypoints\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    Nothing\n",
    "    '''\n",
    "    def detect_waypoint_collision(self, game_car, WAYPOINTS):\n",
    "        for waypoint in WAYPOINTS:\n",
    "            if game_car.colliderect(waypoint):\n",
    "                if waypoint in Car.expired_waypoints: break\n",
    "                pygame.event.post(pygame.event.Event(REWARD))\n",
    "                Car.expired_waypoints.append(waypoint)\n",
    "        # reset if we have hit every waypoint\n",
    "        if len(Car.expired_waypoints) == len(WAYPOINTS):\n",
    "            Car.expired_waypoints = []\n",
    "\n",
    "    '''\n",
    "    Function to simulate laserscan\n",
    "    first laser will point straight ahead of the car, then\n",
    "    increments by 2pi/num_laserscan \n",
    "    -1 is out of range\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    WALLS:  List of all our line segment walls\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    List of laserscan measurements\n",
    "    (x,y) position of the impact of laser to obstacle\n",
    "    '''\n",
    "    def get_laserscan(self, WALLS):\n",
    "        num     = Car.num_laserscan\n",
    "        angle   = Car.ang\n",
    "        # Use line intersection formula\n",
    "        x1 = Car.x\n",
    "        y1 = Car.y\n",
    "        laserscan   = []\n",
    "        impactxy    = []\n",
    "        for i in range(0, num):\n",
    "            # All variables for Line intersection\n",
    "            x2 = Car.x+(Car.laserscan_dist*cos(angle))\n",
    "            y2 = Car.y+(Car.laserscan_dist*sin(angle))\n",
    "            angle += ((2*pi) / num)\n",
    "            intersect = False\n",
    "            for wall in WALLS:\n",
    "                for x3,y3,x4,y4 in wall:\n",
    "                    denom  = (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4)\n",
    "                    if denom == 0: # if denom 0, lines parallel so never intersect\n",
    "                        continue\n",
    "                    t1  = (x1-x3)*(y3-y4) - (y1-y3)*(x3-x4)\n",
    "                    t   = t1/denom\n",
    "                    u1  = (x1-x3)*(y1-y2) - (y1-y3)*(x1-x2)\n",
    "                    u   = u1/denom\n",
    "                    if 0<=t and t<=1 and 0<=u and u<=1: # Test to see if intersection exists\n",
    "                        Px = x1+(t*(x2-x1))\n",
    "                        Py = y1+(t*(y2-y1))\n",
    "                        impactxy.append((Px, Py))\n",
    "                        # Calculate distance between laserscan origin and intersection\n",
    "                        dist = sqrt((Px-Car.x)**2 + (Py-Car.y)**2)\n",
    "                        laserscan.append(dist)\n",
    "                        intersect = True\n",
    "                        break # Stop checking for wall intersection if we already found one\n",
    "                if intersect:\n",
    "                    break\n",
    "            if not intersect:\n",
    "                laserscan.append(-1)\n",
    "\n",
    "        return laserscan, impactxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SDCEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episodes = 10\n",
    "#for episode in range(1, episodes+1):\n",
    "    #state = env.reset()\n",
    "    #done = False\n",
    "    #score = 0 \n",
    "    \n",
    "    #while not done:\n",
    "        #env.render()\n",
    "        #action = env.action_space.sample()\n",
    "        #n_state, reward, done, info = env.step(action)\n",
    "        #score+=reward\n",
    "    #print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states  = env.observation_space.shape\n",
    "#states  = env.observation_space.shape[0] # Use this if using flatten layer\n",
    "num_actions = env.action_space.n\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model(states, num_actions):\n",
    "    model = Sequential()\n",
    "    #model.add(Flatten(input_shape=(1, states)))\n",
    "    model.add(Dense(24, activation='relu', input_shape=(env.num_obs,), batch_size=batch_size))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del model_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = env.batch_size  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "episodes = 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model(states, num_actions)\n",
    "model.summary()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model(states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam speeds up training\n",
    "optimizer = Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for n training steps\n",
    "for i in range(0, episodes):\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        #env.render() # Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesave = 'models/'+str(episodes)+'_episodes'\n",
    "target_filesave = 'models/target_'+str(episodes)+'_episodes'\n",
    "model.save(filesave)\n",
    "model_target.save(target_filesave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reload and use a trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/75000_episodes'\n",
    "model = load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SDCEnv()\n",
    "for episode in range(0, 10):\n",
    "    state = np.array(env.reset())\n",
    "    for step in range(0, 1000):\n",
    "        env.render()\n",
    "        # Predict action Q-values\n",
    "        # From environment state\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = model(state_tensor, training=False)\n",
    "        # Take best action\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        state = state_next\n",
    "        Car.reward += reward\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
